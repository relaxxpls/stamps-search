{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "Running on public URL: https://cab2edede351e79b8a.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://cab2edede351e79b8a.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Any, List, Dict, Optional, Tuple\n",
    "import os\n",
    "\n",
    "import gradio as gr\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from transformers import pipeline, CLIPProcessor, CLIPModel\n",
    "from qdrant_client import QdrantClient\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Initialize Qdrant client\n",
    "qdrant_client = QdrantClient(\"https://qdrant.fijit.club:443\")\n",
    "collection_name = \"stamps2\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class BoundingBox:\n",
    "    xmin: int\n",
    "    ymin: int\n",
    "    xmax: int\n",
    "    ymax: int\n",
    "\n",
    "    @property\n",
    "    def xyxy(self) -> List[float]:\n",
    "        return [self.xmin, self.ymin, self.xmax, self.ymax]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DetectionResult:\n",
    "    score: float\n",
    "    label: str\n",
    "    box: BoundingBox\n",
    "    mask: Optional[np.array] = None\n",
    "\n",
    "    @classmethod\n",
    "    def from_dict(cls, detection_dict: Dict) -> \"DetectionResult\":\n",
    "        return cls(\n",
    "            score=detection_dict[\"score\"],\n",
    "            label=detection_dict[\"label\"],\n",
    "            box=BoundingBox(\n",
    "                xmin=detection_dict[\"box\"][\"xmin\"],\n",
    "                ymin=detection_dict[\"box\"][\"ymin\"],\n",
    "                xmax=detection_dict[\"box\"][\"xmax\"],\n",
    "                ymax=detection_dict[\"box\"][\"ymax\"],\n",
    "            ),\n",
    "        )\n",
    "\n",
    "\n",
    "def detect(\n",
    "    image: Image.Image,\n",
    "    labels: List[str],\n",
    "    threshold: float = 0.3,\n",
    "    detector_id: Optional[str] = \"IDEA-Research/grounding-dino-tiny\",\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Use Grounding DINO to detect a set of labels in an image in a zero-shot fashion.\n",
    "    \"\"\"\n",
    "\n",
    "    object_detector = pipeline(\n",
    "        model=detector_id, task=\"zero-shot-object-detection\", device=device\n",
    "    )\n",
    "\n",
    "    labels = [label if label.endswith(\".\") else label + \".\" for label in labels]\n",
    "\n",
    "    results = object_detector(image, candidate_labels=labels, threshold=threshold)\n",
    "    results = [DetectionResult.from_dict(result) for result in results]\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def detect_stamps(image: Image.Image) -> List[Tuple[Image.Image, DetectionResult]]:\n",
    "    \"\"\"\n",
    "    Return a list of cropped images and their corresponding detection results.\n",
    "    \"\"\"\n",
    "\n",
    "    detections = detect(\n",
    "        image,\n",
    "        [\"a postage stamp.\"],\n",
    "        threshold=0.1,\n",
    "        detector_id=\"IDEA-Research/grounding-dino-base\",\n",
    "    )\n",
    "\n",
    "    cropped_images = []\n",
    "    for detection in detections:\n",
    "        box = detection.box\n",
    "        cropped_image = image.crop(box.xyxy)\n",
    "        cropped_images.append((cropped_image, detection))\n",
    "\n",
    "    return cropped_images\n",
    "\n",
    "\n",
    "def find_similar_stamps(\n",
    "    image: Image.Image, num_matches: int = 5\n",
    ") -> List[Dict[str, Any]]:\n",
    "    # Load the CLIP model\n",
    "    model_name = \"openai/clip-vit-large-patch14\"\n",
    "\n",
    "    model = CLIPModel.from_pretrained(model_name)\n",
    "    model.to(device)\n",
    "    processor = CLIPProcessor.from_pretrained(\n",
    "        model_name, clean_up_tokenization_spaces=True\n",
    "    )\n",
    "\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "    inputs.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        embeddings = model.get_image_features(**inputs)\n",
    "\n",
    "    embedding = embeddings.cpu().squeeze().numpy()\n",
    "\n",
    "    # Use Qdrant to find similar stamps\n",
    "    results = qdrant_client.query_points(\n",
    "        collection_name, query=embedding, limit=num_matches\n",
    "    )\n",
    "\n",
    "    similar_images = [{\"idx\": hit.id, \"distance\": hit.score} for hit in results.points]\n",
    "\n",
    "    return similar_images\n",
    "\n",
    "\n",
    "def resize_image(image: Image.Image, max_size: int = 200) -> Image.Image:\n",
    "    \"\"\"Resize image while maintaining aspect ratio\"\"\"\n",
    "    ratio = max_size / max(image.size)\n",
    "    new_size = tuple([int(x * ratio) for x in image.size])\n",
    "    return image.resize(new_size, Image.Resampling.LANCZOS)\n",
    "\n",
    "\n",
    "def identify_stamps(image_np: np.ndarray) -> List[List[Image.Image]]:\n",
    "    \"\"\"\n",
    "    Identify postage stamps in an image and visualize the results.\n",
    "    \"\"\"\n",
    "\n",
    "    dataset_dir = \"./data/images\"  # Update this path to your dataset directory\n",
    "    image = Image.fromarray(image_np)\n",
    "    detected_stamps = detect_stamps(image)\n",
    "\n",
    "    results = []\n",
    "    for i, (cropped_image, _) in enumerate(detected_stamps):\n",
    "        similar_images = find_similar_stamps(cropped_image)\n",
    "\n",
    "        # Add the detected stamp\n",
    "        results.append((np.array(cropped_image), f\"[{i + 1}] Detected Stamp\"))\n",
    "\n",
    "        # Add similar stamps\n",
    "        for j, similar in enumerate(similar_images):\n",
    "            similar_image_path = os.path.join(dataset_dir, f\"{similar['idx']}.jpg\")\n",
    "            similar_image = Image.open(similar_image_path).convert(\"RGB\")\n",
    "\n",
    "            similar_caption = (\n",
    "                f\"[{i + 1}] Similar {j + 1} (Distance: {similar['distance']:.4f})\"\n",
    "            )\n",
    "            results.append((np.array(similar_image), similar_caption))\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def create_gradio_interface():\n",
    "    with gr.Blocks() as demo:\n",
    "        gr.Markdown(\"# Stamp Identifier\")\n",
    "        with gr.Row():\n",
    "            input_image = gr.Image()\n",
    "            output_gallery = gr.Gallery(label=\"Detected Stamps and Similar Images\", object_fit=\"contain\")\n",
    "\n",
    "        submit_btn = gr.Button(\"Identify Stamps\")\n",
    "\n",
    "        submit_btn.click(fn=identify_stamps, inputs=[input_image], outputs=[output_gallery])\n",
    "\n",
    "    return demo\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo = create_gradio_interface()\n",
    "    demo.launch(share=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_path = \"./data/album/album6.png\"\n",
    "# dataset_dir = \"./data/images\"\n",
    "# identify_stamps(image_path, dataset_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
